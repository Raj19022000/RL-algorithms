{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from collections import deque\n",
    "from queue import Queue \n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self,learning_rate,input_dims,h1,n_actions):\n",
    "        super(Policy,self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dims,h1)\n",
    "        self.linear2 = nn.Linear(h1,n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return F.softmax(x,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cart_agent():\n",
    "    def __init__(self,gamma,l_r,input_dims,n_actions,save=False):\n",
    "        self.gamma = gamma\n",
    "        self.policy = Policy(learning_rate = l_r,input_dims=input_dims,h1=128,\n",
    "                            n_actions=n_actions).to(device)\n",
    "        self.states = np.array([[0,0,0,0]])\n",
    "        self.actions = np.array([])\n",
    "        self.rewards = np.array([])\n",
    "        self.returns = 0\n",
    "    def choose_action(self,obs):\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        with torch.no_grad():\n",
    "            prob = self.policy.forward(obs)\n",
    "            action = torch.argmax(prob)\n",
    "        return int(action)\n",
    "    def store_trajectory(self,state,action,reward):\n",
    "        self.states = np.append(self.states,state.reshape(1,4),axis=0)\n",
    "        self.actions = np.append(self.actions,action)\n",
    "        self.rewards = np.append(self.rewards,reward)\n",
    "        self.returns += reward * np.power(self.gamma, self.states.size - 1) \n",
    "    def improve(self):\n",
    "        g = self.returns\n",
    "        for i in range(self.actions.size):\n",
    "            s = torch.Tensor(self.states[i+1]).to(device)\n",
    "            a = self.actions[i]\n",
    "            r = self.rewards[i]\n",
    "            self.policy.optimizer.zero_grad()\n",
    "            loss = (np.power(self.gamma,i))*g*torch.log(self.policy.forward(s)[int(a)]).to(device)\n",
    "            loss.backward()\n",
    "            self.policy.optimizer.step()\n",
    "            g = np.divide((g - r),self.gamma)\n",
    "        self.states = np.array([[0,0,0,0]])\n",
    "        self.actions = np.array([])\n",
    "        self.rewards = np.array([])\n",
    "        self.returns = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = cart_agent(gamma=0.99,l_r=0.003,input_dims=4,n_actions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "n_games = 400\n",
    "score = 0\n",
    "best_score = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.0\n",
      "107.5\n",
      "138.33333333333334\n",
      "153.75\n",
      "153.8\n",
      "161.5\n",
      "153.57142857142858\n",
      "159.375\n",
      "163.88888888888889\n",
      "167.5\n",
      "168.9\n",
      "169.5\n",
      "169.5\n",
      "168.6\n",
      "163.6\n",
      "163.5\n",
      "172.9\n",
      "172.9\n",
      "172.9\n",
      "172.9\n",
      "172.0\n",
      "179.7\n",
      "179.7\n",
      "178.3\n",
      "187.9\n",
      "178.7\n",
      "178.7\n",
      "178.7\n",
      "178.7\n",
      "178.7\n",
      "188.4\n",
      "188.4\n",
      "188.4\n",
      "182.5\n",
      "182.5\n",
      "191.8\n",
      "191.8\n",
      "191.8\n",
      "191.8\n",
      "191.8\n",
      "191.8\n",
      "191.8\n",
      "191.8\n",
      "196.0\n",
      "189.0\n",
      "189.0\n",
      "189.0\n",
      "189.0\n",
      "189.0\n",
      "189.0\n",
      "189.0\n",
      "181.0\n",
      "181.0\n",
      "185.0\n",
      "192.0\n",
      "182.7\n",
      "182.7\n",
      "180.8\n",
      "172.6\n",
      "166.8\n",
      "166.8\n",
      "174.8\n",
      "166.4\n",
      "162.3\n",
      "162.3\n",
      "168.5\n",
      "168.5\n",
      "169.7\n",
      "168.9\n",
      "174.7\n",
      "174.7\n",
      "170.8\n",
      "179.2\n",
      "183.3\n",
      "174.3\n",
      "177.4\n",
      "172.6\n",
      "163.3\n",
      "172.3\n",
      "172.3\n",
      "172.3\n",
      "176.2\n",
      "175.0\n",
      "175.0\n",
      "184.0\n",
      "175.5\n",
      "180.3\n",
      "190.3\n",
      "190.3\n",
      "180.5\n",
      "180.5\n",
      "180.5\n",
      "181.7\n",
      "172.1\n",
      "171.5\n",
      "170.8\n",
      "170.8\n",
      "163.8\n",
      "153.3\n",
      "157.6\n",
      "153.7\n",
      "149.4\n",
      "149.4\n",
      "159.0\n",
      "148.9\n",
      "158.1\n",
      "158.1\n",
      "155.6\n",
      "166.1\n",
      "171.1\n",
      "168.7\n",
      "166.5\n",
      "166.5\n",
      "166.5\n",
      "167.6\n",
      "167.6\n",
      "157.3\n",
      "158.7\n",
      "156.9\n",
      "147.6\n",
      "153.9\n",
      "148.7\n",
      "138.8\n",
      "128.9\n",
      "128.1\n",
      "121.7\n",
      "121.4\n",
      "129.5\n",
      "127.9\n",
      "131.5\n",
      "130.4\n",
      "141.3\n",
      "143.0\n",
      "147.1\n",
      "149.6\n",
      "144.9\n",
      "155.5\n",
      "150.1\n",
      "153.5\n",
      "151.6\n",
      "152.7\n",
      "153.5\n",
      "151.8\n",
      "147.9\n",
      "154.5\n",
      "165.6\n",
      "161.8\n",
      "167.2\n",
      "164.3\n",
      "169.0\n",
      "157.8\n",
      "157.8\n",
      "167.7\n",
      "177.4\n",
      "178.3\n",
      "169.0\n",
      "172.8\n",
      "172.8\n",
      "174.9\n",
      "167.8\n",
      "174.4\n",
      "174.4\n",
      "173.0\n",
      "173.0\n",
      "173.4\n",
      "173.3\n",
      "173.3\n",
      "169.8\n",
      "170.2\n",
      "180.7\n",
      "185.3\n",
      "185.3\n",
      "186.7\n",
      "186.7\n",
      "185.8\n",
      "195.2\n",
      "195.2\n",
      "198.7\n",
      "192.7\n",
      "187.5\n",
      "179.2\n",
      "179.2\n",
      "171.3\n",
      "171.3\n",
      "163.1\n",
      "158.0\n",
      "147.5\n",
      "138.1\n",
      "135.1\n",
      "134.4\n",
      "141.2\n",
      "141.2\n",
      "140.6\n",
      "140.6\n",
      "146.6\n",
      "142.8\n",
      "149.1\n",
      "156.1\n",
      "165.5\n",
      "171.4\n",
      "171.6\n",
      "163.0\n",
      "171.5\n",
      "163.6\n",
      "162.6\n",
      "164.2\n",
      "157.3\n",
      "151.4\n",
      "150.5\n",
      "150.5\n",
      "144.6\n",
      "142.9\n",
      "142.9\n",
      "147.4\n",
      "140.6\n",
      "143.2\n",
      "154.3\n",
      "161.3\n",
      "162.2\n",
      "153.7\n",
      "157.4\n",
      "167.7\n",
      "166.1\n",
      "167.4\n",
      "169.6\n",
      "162.6\n",
      "158.5\n",
      "154.5\n",
      "143.9\n",
      "152.4\n",
      "144.6\n",
      "135.1\n",
      "130.7\n",
      "132.8\n",
      "141.5\n",
      "145.5\n",
      "142.3\n",
      "147.6\n",
      "158.2\n",
      "153.4\n",
      "164.7\n",
      "169.3\n",
      "166.6\n",
      "166.6\n",
      "166.6\n",
      "164.2\n",
      "166.5\n",
      "161.8\n",
      "155.4\n",
      "158.7\n",
      "151.3\n",
      "146.5\n",
      "145.5\n",
      "139.4\n",
      "139.4\n",
      "140.7\n",
      "138.2\n",
      "142.9\n",
      "149.3\n",
      "139.5\n",
      "139.9\n",
      "147.7\n",
      "155.7\n",
      "151.1\n",
      "151.1\n",
      "150.7\n",
      "148.3\n",
      "146.4\n",
      "136.3\n",
      "138.9\n",
      "134.6\n",
      "128.5\n",
      "130.2\n",
      "129.8\n",
      "126.4\n",
      "124.1\n",
      "126.9\n",
      "119.3\n",
      "118.1\n",
      "123.1\n",
      "134.4\n",
      "134.3\n",
      "128.8\n",
      "129.6\n",
      "127.7\n",
      "129.3\n",
      "136.4\n",
      "136.2\n",
      "142.0\n",
      "145.4\n",
      "138.3\n",
      "137.1\n",
      "142.6\n",
      "145.8\n",
      "146.1\n",
      "156.0\n",
      "147.8\n",
      "157.4\n",
      "162.9\n",
      "163.2\n",
      "170.3\n",
      "168.5\n",
      "160.2\n",
      "167.3\n",
      "167.3\n",
      "160.4\n",
      "168.6\n",
      "162.6\n",
      "160.9\n",
      "156.2\n",
      "149.4\n",
      "158.4\n",
      "160.7\n",
      "152.3\n",
      "154.6\n",
      "159.5\n",
      "150.7\n",
      "154.2\n",
      "155.5\n",
      "160.2\n",
      "167.0\n",
      "164.2\n",
      "161.9\n",
      "170.3\n",
      "162.3\n",
      "158.8\n",
      "165.7\n",
      "161.9\n",
      "152.7\n",
      "147.2\n",
      "140.6\n",
      "145.5\n",
      "153.8\n",
      "144.9\n",
      "145.2\n",
      "150.7\n",
      "148.0\n",
      "150.8\n",
      "158.5\n",
      "155.4\n",
      "156.2\n",
      "148.4\n",
      "148.4\n",
      "157.3\n",
      "160.7\n",
      "156.9\n",
      "161.5\n",
      "157.3\n",
      "159.2\n",
      "167.8\n",
      "161.8\n",
      "164.6\n",
      "155.6\n",
      "149.4\n",
      "156.4\n",
      "160.2\n",
      "150.0\n",
      "157.8\n",
      "148.5\n",
      "138.5\n",
      "143.1\n",
      "140.3\n",
      "137.9\n",
      "134.9\n",
      "127.1\n",
      "118.1\n",
      "117.9\n",
      "108.1\n",
      "108.6\n",
      "118.6\n",
      "119.1\n",
      "123.9\n",
      "126.7\n",
      "126.9\n",
      "134.7\n",
      "136.8\n",
      "138.9\n",
      "138.7\n",
      "138.4\n",
      "129.2\n",
      "126.2\n",
      "121.6\n",
      "124.7\n",
      "127.0\n",
      "117.8\n",
      "117.2\n",
      "125.5\n",
      "135.5\n",
      "135.1\n",
      "144.3\n",
      "146.3\n",
      "144.7\n",
      "150.2\n",
      "146.6\n",
      "148.7\n",
      "148.8\n",
      "138.4\n",
      "138.4\n",
      "140.4\n",
      "131.3\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.store_trajectory(state,action,reward)\n",
    "        score += reward\n",
    "        state = next_state\n",
    "    scores.append(score)\n",
    "    agent.improve()\n",
    "    print(np.mean(scores[-10:]))\n",
    "    if np.mean(scores[-10:])>best_score and i>10:\n",
    "        best_score = np.mean(scores[-10:])\n",
    "        torch.save(agent.policy.state_dict(),'/home/raj/My_projects/REINFORCE/CartPole_lowstate(REINFORCE).pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194.0\n",
      "\n",
      "\n",
      "200.0\n",
      "\n",
      "\n",
      "200.0\n",
      "\n",
      "\n",
      "200.0\n",
      "\n",
      "\n",
      "144.0\n",
      "\n",
      "\n",
      "187.6\n"
     ]
    }
   ],
   "source": [
    "n_games = 5\n",
    "scores = []\n",
    "agent.policy.load_state_dict(torch.load('/home/raj/My_projects/REINFORCE/CartPole_lowstate(REINFORCE).pt'))\n",
    "\n",
    "for i in range(n_games):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        env.render()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        state = next_state\n",
    "    print(score)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    scores.append(score)\n",
    "print(np.mean(scores))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
